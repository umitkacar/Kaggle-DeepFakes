{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install /kaggle/input/modelown/facenet_pytorch-2.2.9-py3-none-any.whl\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0,'/kaggle/working/reader/python')\n",
    "import time\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"device:\", device)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def leaf_l1_score(xlist, masklist, ch=None):\n",
    "    loss_list = []\n",
    "    xshape = xlist[0].shape\n",
    "    scores = []\n",
    "    for x, mask in zip(xlist, masklist):\n",
    "        if ch is not None:\n",
    "            score = tf.reduce_mean(tf.reshape(tf.abs(x[:, :, :, ch]), [xshape[0], -1]), axis=1)\n",
    "        else:\n",
    "            score = tf.reduce_mean(tf.reshape(tf.abs(x), [xshape[0], -1]), axis=1)\n",
    "        spoof_score = score * mask[:, 0]\n",
    "        scores.append(spoof_score)\n",
    "    loss = np.sum(np.stack(scores, axis=1), axis=1)\n",
    "    return loss\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, idx, alpha, beta, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        initializer0 = tf.zeros_initializer()\n",
    "        self.v = tf.Variable(initial_value=initializer(shape=(1, input_dim), dtype='float32'),\n",
    "                             trainable=True, name='tru/v/'+idx)\n",
    "        self.mu = tf.Variable(initial_value=initializer0(shape=(1, input_dim), dtype='float32'),\n",
    "                              trainable=True, name='tru/mu/'+idx)\n",
    "        # training hyper-parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # mean, eigenvalue and trace for each mini-batch\n",
    "        self.mu_of_visit = 0\n",
    "        self.eigenvalue = 0.\n",
    "        self.trace = 0.\n",
    "\n",
    "    def call(self, x, mask, training):\n",
    "        norm_v = self.v / (tf.norm(self.v) + 1e-8)\n",
    "        norm_v_t = tf.transpose(norm_v, [1, 0])\n",
    "        num_of_visit = tf.reduce_sum(mask)\n",
    "\n",
    "        if training and num_of_visit > 1:\n",
    "            # use only the visiting samples\n",
    "            index = tf.where(tf.greater(mask[:, 0], tf.constant(0.)))\n",
    "            index_not = tf.where(tf.equal(mask[:, 0], tf.constant(0.)))\n",
    "            x_sub = tf.gather_nd(x, index) - tf.stop_gradient(self.mu)\n",
    "            x_not = tf.gather_nd(x, index_not)\n",
    "            x_sub_t = tf.transpose(x_sub, [1, 0])\n",
    "\n",
    "            # compute the covariance matrix, eigenvalue, and the trace\n",
    "            covar = tf.matmul(x_sub_t, x_sub) / num_of_visit\n",
    "            eigenvalue = tf.reshape(tf.matmul(tf.matmul(norm_v, covar), norm_v_t), [])\n",
    "            trace = tf.linalg.trace(covar)\n",
    "            # compute the route loss\n",
    "            # print(tf.exp(-self.alpha * eigenvalue), self.beta * trace)\n",
    "            route_loss = tf.exp(-self.alpha * eigenvalue) + self.beta * trace\n",
    "            uniq_loss = -tf.reduce_mean(tf.square(tf.matmul(x_sub, norm_v_t))) + \\\n",
    "                         tf.reduce_mean(tf.square(tf.matmul(x_not, norm_v_t)))\n",
    "            # compute mean and response for this batch\n",
    "            self.mu_of_visit = tf.reduce_mean(x_sub, axis=0, keepdims=True)\n",
    "            self.eigenvalue = eigenvalue\n",
    "            self.trace = trace\n",
    "            x -= tf.stop_gradient(self.mu_of_visit)\n",
    "            route_value = tf.matmul(x, norm_v_t)\n",
    "        else:\n",
    "            self.mu_of_visit = self.mu\n",
    "            self.eigenvalue = 0.\n",
    "            self.trace = 0.\n",
    "            x -= self.mu\n",
    "            route_value = tf.matmul(x, norm_v_t)\n",
    "            route_loss = 0.\n",
    "            uniq_loss = 0.\n",
    "\n",
    "        return route_value, route_loss, uniq_loss\n",
    "\n",
    "class Downsample(tf.keras.Model):\n",
    "    def __init__(self, filters, size, padding='SAME', apply_batchnorm=True):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        filters = int(filters)\n",
    "        self.conv1 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=2,\n",
    "                                   padding=padding,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "        if self.apply_batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.conv1(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(tf.keras.Model):\n",
    "    def __init__(self, filters, size, apply_dropout=False):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.apply_dropout = apply_dropout\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        filters = int(filters)\n",
    "        self.up_conv = tf.keras.layers.Conv2DTranspose(filters,\n",
    "                                                       (size, size),\n",
    "                                                       strides=2,\n",
    "                                                       padding='same',\n",
    "                                                       kernel_initializer=initializer,\n",
    "                                                       use_bias=False)\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        if self.apply_dropout:\n",
    "            self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.up_conv(x)\n",
    "        x = self.batchnorm(x, training=training)\n",
    "        if self.apply_dropout:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "class Conv(tf.keras.Model):\n",
    "    def __init__(self, filters, size, stride=1, activation=True, padding='SAME', apply_batchnorm=True):\n",
    "        super(Conv, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        self.activation = activation\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        filters = int(filters)\n",
    "        self.conv1 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=stride,\n",
    "                                   padding=padding,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "        if self.apply_batchnorm:\n",
    "            self.batchnorm = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.conv1(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "        if self.activation:\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "class Dense(tf.keras.Model):\n",
    "    def __init__(self, filters, activation=True, apply_batchnorm=True, apply_dropout=False):\n",
    "        super(Dense, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        self.activation = activation\n",
    "        self.apply_dropout = apply_dropout\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        filters = int(filters)\n",
    "        self.dense = layers.Dense(filters,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  use_bias=False)\n",
    "        if self.apply_batchnorm:\n",
    "            self.batchnorm = layers.BatchNormalization()\n",
    "        if self.apply_dropout:\n",
    "            self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.dense(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.batchnorm(x, training=training)\n",
    "        if self.activation:\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "        if self.apply_dropout:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "class CRU(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, filters, size=3, stride=2, apply_batchnorm=True):\n",
    "        super(CRU, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        self.stride = stride\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "        filters = int(filters)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=1,\n",
    "                                   padding='SAME',\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "        self.conv2 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=1,\n",
    "                                   padding='SAME',\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "        self.conv3 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=1,\n",
    "                                   padding='SAME',\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "        self.conv4 = layers.Conv2D(filters,\n",
    "                                   (size, size),\n",
    "                                   strides=1,\n",
    "                                   padding='SAME',\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   use_bias=False)\n",
    "\n",
    "        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm4 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x, training):\n",
    "        # first residual block\n",
    "        _x = self.conv1(x)\n",
    "        _x = self.batchnorm1(_x, training=training)\n",
    "        _x = tf.nn.leaky_relu(_x)\n",
    "        _x = self.conv2(_x)\n",
    "        _x = self.batchnorm2(_x, training=training)\n",
    "        _x  = x + _x\n",
    "        x  = tf.nn.leaky_relu(_x)\n",
    "\n",
    "        # second residual block\n",
    "        _x = self.conv3(x)\n",
    "        _x = self.batchnorm3(_x, training=training)\n",
    "        _x = tf.nn.leaky_relu(_x)\n",
    "        _x = self.conv4(_x)\n",
    "        _x = self.batchnorm4(_x, training=training)\n",
    "        _x = x + _x\n",
    "        x = tf.nn.leaky_relu(_x)\n",
    "\n",
    "        if self.stride > 1:\n",
    "            x = tf.nn.max_pool(x, 3, 2, padding='SAME')\n",
    "        return x\n",
    "\n",
    "class TRU(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, filters, idx, alpha=1e-3, beta=1e-4, size=3, apply_batchnorm=True):\n",
    "        super(TRU, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        # variables\n",
    "        self.conv1 = Downsample(filters, size)\n",
    "        self.conv2 = Downsample(filters, size)\n",
    "        self.conv3 = Downsample(filters, size)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.project = Linear(idx, alpha, beta, input_dim=2048)\n",
    "\n",
    "\n",
    "    def call(self, x, mask, training):\n",
    "        # Downsampling\n",
    "        x_small = self.conv1(x, training=training)\n",
    "        depth = 0\n",
    "        if x_small.shape[1] > 16:\n",
    "            x_small = self.conv2(x_small, training=training)\n",
    "            depth += 1\n",
    "            if x_small.shape[1] > 16:\n",
    "                x_small = self.conv3(x_small, training=training)\n",
    "                depth += 1\n",
    "        x_small_shape = x_small.shape\n",
    "        x_flatten = self.flatten(tf.nn.avg_pool(x_small, ksize=3, strides=2, padding='SAME'))\n",
    "\n",
    "        # PCA Projection\n",
    "        route_value, route_loss, uniq_loss = self.project(x_flatten, mask, training=training)\n",
    "\n",
    "        # Generate the splitting mask\n",
    "        mask_l = mask * tf.cast(tf.greater_equal(route_value, tf.constant(0.)), tf.float32)\n",
    "        mask_r = mask * tf.cast(tf.less(route_value, tf.constant(0.)), tf.float32)\n",
    "\n",
    "        return [mask_l, mask_r], route_value, [route_loss, uniq_loss]\n",
    "\n",
    "class SFL(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, filters, size=3, apply_batchnorm=True):\n",
    "        super(SFL, self).__init__()\n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        # depth map\n",
    "        self.cru1 = CRU(filters, size, stride=1)\n",
    "        self.conv1 = Conv(2, size, activation=False, apply_batchnorm=False)\n",
    "\n",
    "        # class\n",
    "        self.conv2 = Downsample(filters*1, size)\n",
    "        self.conv3 = Downsample(filters*1, size)\n",
    "        self.conv4 = Downsample(filters*2, size)\n",
    "        self.conv5 = Downsample(filters*4, 4, padding='VALID')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = Dense(256)\n",
    "        self.fc2 = Dense(1, activation=False, apply_batchnorm=False)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        # depth map branch\n",
    "        xd = self.cru1(x)\n",
    "        xd = self.conv1(xd)\n",
    "        dmap = tf.nn.sigmoid(xd)\n",
    "        # class branch\n",
    "        x = self.conv2(x)  # 16*16*32\n",
    "        x = self.conv3(x)  # 8*8*64\n",
    "        x = self.conv4(x)  # 4*4*128\n",
    "        x = self.conv5(x)  # 1*1*256\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc1(x)\n",
    "        cls = self.fc2(x)\n",
    "        return dmap, cls\n",
    "\n",
    "############################################################\n",
    "#  Deep Tree Network (DTN)\n",
    "############################################################\n",
    "\n",
    "class DTN(tf.keras.models.Model):\n",
    "    def __init__(self, filters):\n",
    "        super(DTN, self).__init__()\n",
    "        \n",
    "        TRU_PARAMETERS = {\n",
    "        \"alpha\": 1e-3,\n",
    "        \"beta\": 1e-2,\n",
    "        \"mu_update_rate\": 1e-3,\n",
    "        }\n",
    "        \n",
    "        layer = [1, 2, 4, 8, 16]\n",
    "        self.conv1 = Conv(filters, 5, apply_batchnorm=False)\n",
    "        # CRU\n",
    "        self.cru0 = CRU(filters)\n",
    "        self.cru1 = CRU(filters)\n",
    "        self.cru2 = CRU(filters)\n",
    "        self.cru3 = CRU(filters)\n",
    "        self.cru4 = CRU(filters)\n",
    "        self.cru5 = CRU(filters)\n",
    "        self.cru6 = CRU(filters)\n",
    "        # TRU\n",
    "        alpha = TRU_PARAMETERS['alpha']\n",
    "        beta = TRU_PARAMETERS['beta']\n",
    "        self.tru0 = TRU(filters, '1', alpha, beta)\n",
    "        self.tru1 = TRU(filters, '2', alpha, beta)\n",
    "        self.tru2 = TRU(filters, '3', alpha, beta)\n",
    "        self.tru3 = TRU(filters, '4', alpha, beta)\n",
    "        self.tru4 = TRU(filters, '5', alpha, beta)\n",
    "        self.tru5 = TRU(filters, '6', alpha, beta)\n",
    "        self.tru6 = TRU(filters, '7', alpha, beta)\n",
    "        # SFL\n",
    "        self.sfl0 = SFL(filters)\n",
    "        self.sfl1 = SFL(filters)\n",
    "        self.sfl2 = SFL(filters)\n",
    "        self.sfl3 = SFL(filters)\n",
    "        self.sfl4 = SFL(filters)\n",
    "        self.sfl5 = SFL(filters)\n",
    "        self.sfl6 = SFL(filters)\n",
    "        self.sfl7 = SFL(filters)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, label, training):\n",
    "        if training:\n",
    "            mask_spoof = label\n",
    "            mask_live = 1 - label\n",
    "        else:\n",
    "            mask_spoof = tf.ones_like(label)\n",
    "            mask_live = tf.zeros_like(label)\n",
    "        ''' Tree Level 1 '''\n",
    "        x = self.conv1(x, training)\n",
    "        x_cru0 = self.cru0(x)\n",
    "        x_tru0, route_value0, tru0_loss = self.tru0(x_cru0, mask_spoof, training)\n",
    "\n",
    "        ''' Tree Level 2 '''\n",
    "        x_cru00 = self.cru1(x_cru0, training)\n",
    "        x_cru01 = self.cru2(x_cru0, training)\n",
    "        x_tru00, route_value00, tru00_loss = self.tru1(x_cru00, x_tru0[0], training)\n",
    "        x_tru01, route_value01, tru01_loss = self.tru2(x_cru01, x_tru0[1], training)\n",
    "\n",
    "        ''' Tree Level 3 '''\n",
    "        x_cru000 = self.cru3(x_cru00, training)\n",
    "        x_cru001 = self.cru4(x_cru00, training)\n",
    "        x_cru010 = self.cru5(x_cru01, training)\n",
    "        x_cru011 = self.cru6(x_cru01, training)\n",
    "        x_tru000, route_value000, tru000_loss = self.tru3(x_cru000, x_tru00[0], training)\n",
    "        x_tru001, route_value001, tru001_loss = self.tru4(x_cru001, x_tru00[1], training)\n",
    "        x_tru010, route_value010, tru010_loss = self.tru5(x_cru010, x_tru01[0], training)\n",
    "        x_tru011, route_value011, tru011_loss = self.tru6(x_cru011, x_tru01[1], training)\n",
    "\n",
    "        ''' Tree Level 4 '''\n",
    "        map0, cls0 = self.sfl0(x_cru000, training)\n",
    "        map1, cls1 = self.sfl1(x_cru000, training)\n",
    "        map2, cls2 = self.sfl2(x_cru001, training)\n",
    "        map3, cls3 = self.sfl3(x_cru001, training)\n",
    "        map4, cls4 = self.sfl4(x_cru010, training)\n",
    "        map5, cls5 = self.sfl5(x_cru010, training)\n",
    "        map6, cls6 = self.sfl6(x_cru011, training)\n",
    "        map7, cls7 = self.sfl7(x_cru011, training)\n",
    "        ''' Output '''\n",
    "        maps = [map0, map1, map2, map3, map4, map5, map6, map7]\n",
    "        clss = [cls0, cls1, cls2, cls3, cls4, cls5, cls6, cls7]\n",
    "        route_value = [route_value0, route_value00, route_value01,\n",
    "                       route_value000, route_value001, route_value010, route_value011]\n",
    "        x_tru0000 = tf.concat([x_tru000[0], mask_live], axis=1)\n",
    "        x_tru0001 = tf.concat([x_tru000[1], mask_live], axis=1)\n",
    "        x_tru0010 = tf.concat([x_tru001[0], mask_live], axis=1)\n",
    "        x_tru0011 = tf.concat([x_tru001[1], mask_live], axis=1)\n",
    "        x_tru0100 = tf.concat([x_tru010[0], mask_live], axis=1)\n",
    "        x_tru0101 = tf.concat([x_tru010[1], mask_live], axis=1)\n",
    "        x_tru0110 = tf.concat([x_tru011[0], mask_live], axis=1)\n",
    "        x_tru0111 = tf.concat([x_tru011[1], mask_live], axis=1)\n",
    "        leaf_node_mask = [x_tru0000, x_tru0001, x_tru0010, x_tru0011, x_tru0100, x_tru0101, x_tru0110, x_tru0111]\n",
    "\n",
    "        return maps, clss, route_value, leaf_node_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMTCNN_orj(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
    "\n",
    "        faces = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            box_ind = int(i / self.stride)\n",
    "            if boxes[box_ind] is None:\n",
    "                continue\n",
    "            for box in boxes[box_ind]:\n",
    "                box = [int(b) for b in box]\n",
    "                #faces.append(frame[box[1]:box[3], box[0]:box[2]])\n",
    "                image_rgb = frame[box[1]:box[3], box[0]:box[2]]\n",
    "                if (len(image_rgb) > 0) and (image_rgb.shape[0] > 0) and (image_rgb.shape[1] > 0):\n",
    "                    image_rgb = cv2.resize(image_rgb, (256,256))\n",
    "                    image_hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
    "                    image_rgb = image_rgb / 255\n",
    "                    image_hsv = image_hsv / 255\n",
    "                    image = np.concatenate([image_rgb, image_hsv], axis=2)\n",
    "                    faces.append(image)\n",
    "                    \n",
    "        return faces\n",
    "\n",
    "#Full Resolution settings\n",
    "fast_mtcnn_orj = FastMTCNN_orj(\n",
    "    stride=10,\n",
    "    resize=1,\n",
    "    margin=1,\n",
    "    factor=0.5,\n",
    "    keep_all=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# fast_mtcnn = FastMTCNN(\n",
    "#     stride=4,\n",
    "#     resize=0.5,\n",
    "#     margin=14,\n",
    "#     factor=0.5,\n",
    "#     keep_all=True,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f972c485860>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DTN model checkpoint\n",
    "## LOAD DTN Model and Checkpoint\n",
    "dtn = DTN(32)\n",
    "dtn_op = tf.compat.v1.train.AdamOptimizer(0.0005, beta1=0.5)\n",
    "checkpoint = tf.train.Checkpoint(dtn=dtn,dtn_optimizer=dtn_op)\n",
    "checkpoint.restore(\"../input/modelown/ckpt-269\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dir = \"../input/deepfake-detection-challenge/test_videos/\"\n",
    "#filenames = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])[:20]\n",
    "filenames = glob.glob('../input/deepfake-detection-challenge/test_videos/*.mp4')\n",
    "#filenames = glob.glob('../input/deepfake-detection-challenge/train_sample_videos/*.mp4')\n",
    "files = [os.path.basename(f[:-4]) for f in filenames]\n",
    "pd_video_names = pd.DataFrame({'file': files,'VIDEO_FILE': filenames})\n",
    "pd_video_names.sort_values(by=['file'], inplace=True)\n",
    "#pd.options.display.max_colwidth = 75\n",
    "#print(pd_video_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972d141d30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c68f908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6a62e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6b4c88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c64b668>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c665048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6739e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972d141d30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c68f908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6a62e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6b4c88>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c64b668>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c665048>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n",
      "WARNING: AutoGraph could not transform <bound method Linear.call of <__main__.Linear object at 0x7f972c6739e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unexpected indent (<unknown>, line 29)\n"
     ]
    }
   ],
   "source": [
    "pd_model_labels = pd_video_names.copy()\n",
    "pd_model_labels['group_label'] = 0.5\n",
    "\n",
    "ls_files = []\n",
    "ls_model_labels = []\n",
    "ls_dmap_score_mean = []\n",
    "ls_dmap_score_max = []\n",
    "ls_cls_score_mean = []\n",
    "ls_cls_score_max = []\n",
    "\n",
    "jump_frame = 1\n",
    "last_frame = 200\n",
    "\n",
    "fBatch = 26\n",
    "mBatch = 12\n",
    "\n",
    "extended_label = np.ones(shape=(mBatch,1))\n",
    "label_ts = tf.convert_to_tensor(extended_label, dtype=tf.float32)\n",
    "\n",
    "ERROR_DEFAULT_VALUE =  0.5 # ERROR_DEFAULT_VALUE\n",
    "for each in pd_video_names.itertuples():\n",
    "    \n",
    "    frames = []\n",
    "    dmap_score_temp = [] \n",
    "    cls_score_temp = []\n",
    "    \n",
    "    try:\n",
    "        v_cap = cv2.VideoCapture(each.VIDEO_FILE)\n",
    "        #videoFrameNumber = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        i = 0\n",
    "        while(v_cap.isOpened()):\n",
    "            #v_cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = v_cap.read()\n",
    "            if (ret == True) and (i < last_frame):\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                if (len(frames) >= fBatch):   \n",
    "                    faces = fast_mtcnn_orj(frames)\n",
    "                    frames = []\n",
    "                    if faces and (len(faces) > mBatch): # empty control\n",
    "                        faceStack = np.stack(faces, axis=0)\n",
    "                        mLast  =  int(faceStack.shape[0]) - mBatch\n",
    "                        for batch in range(0,mLast,mBatch):\n",
    "                            image_ts = tf.convert_to_tensor(faceStack[batch:batch+mBatch,:,:,:], dtype=tf.float32)\n",
    "                            with tf.GradientTape() as tape:\n",
    "                                dmap_pred, cls_pred, route_value, leaf_node_mask = dtn(image_ts, label_ts, False)\n",
    "\n",
    "                            dmap_s = leaf_l1_score(dmap_pred, leaf_node_mask)\n",
    "                            cls_s = leaf_l1_score(cls_pred, leaf_node_mask)\n",
    "                            dmap_score_temp.extend(dmap_s)\n",
    "                            cls_score_temp.extend(cls_s)\n",
    "                        faceStack = []\n",
    "\n",
    "\n",
    "            else:\n",
    "                break\n",
    "            i = i + jump_frame\n",
    "\n",
    "        v_cap.release()\n",
    "        if dmap_score_temp and cls_score_temp:\n",
    "            dmap_score_max = np.max(dmap_score_temp)\n",
    "            dmap_score_mean = np.mean(dmap_score_temp)\n",
    "            cls_score_mean = np.mean(cls_score_temp)\n",
    "\n",
    "            if cls_score_mean > 0.90:\n",
    "                last_score = 0.93\n",
    "            elif cls_score_mean > 0.5 and dmap_score_max > 0.3:\n",
    "                last_score = 0.93\n",
    "            elif dmap_score_mean > 0.2 and dmap_score_max > 0.3:\n",
    "                last_score = 0.5\n",
    "            elif cls_score_mean < 0.5 and dmap_score_mean < 0.2 and dmap_score_max < 0.3:\n",
    "                last_score = 0.07\n",
    "            else:\n",
    "                last_score = 0.5\n",
    "        else:\n",
    "            dmap_score_max = ERROR_DEFAULT_VALUE\n",
    "            dmap_score_mean = ERROR_DEFAULT_VALUE\n",
    "            cls_score_mean = ERROR_DEFAULT_VALUE\n",
    "            last_score = ERROR_DEFAULT_VALUE \n",
    "        \n",
    "    except:\n",
    "        print(\"File not opened!!!\")\n",
    "        dmap_score_max = ERROR_DEFAULT_VALUE\n",
    "        dmap_score_mean = ERROR_DEFAULT_VALUE\n",
    "        cls_score_mean = ERROR_DEFAULT_VALUE\n",
    "        last_score = ERROR_DEFAULT_VALUE    \n",
    "\n",
    "    ls_files.append(each.file)\n",
    "    ls_dmap_score_mean.append(dmap_score_mean)\n",
    "    ls_dmap_score_max.append(dmap_score_max)  \n",
    "    ls_cls_score_mean.append(cls_score_mean) \n",
    "    ls_model_labels.append(last_score)\n",
    "    \n",
    "pd_model_labels = pd.DataFrame({'file': ls_files,\n",
    "                                'dmap_mean':ls_dmap_score_mean,\n",
    "                                'dmap_max':ls_dmap_score_max,\n",
    "                                'cls_mean':ls_cls_score_mean,\n",
    "                                'model_label': ls_model_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>ztyvglkcsf.mp4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>zuwwbbusgl.mp4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>zxacihctqp.mp4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>zyufpqvpyu.mp4</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>zzmgnglanj.mp4</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  label\n",
       "0    aassnaulhq.mp4   0.93\n",
       "1    aayfryxljh.mp4   0.50\n",
       "2    acazlolrpz.mp4   0.50\n",
       "3    adohdulfwb.mp4   0.07\n",
       "4    ahjnxtiamx.mp4   0.93\n",
       "..              ...    ...\n",
       "395  ztyvglkcsf.mp4   0.07\n",
       "396  zuwwbbusgl.mp4   0.07\n",
       "397  zxacihctqp.mp4   0.07\n",
       "398  zyufpqvpyu.mp4   0.93\n",
       "399  zzmgnglanj.mp4   0.07\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd_model_labels[['file','model_label']].copy()\n",
    "submission['file'] = submission['file'].astype(str) + '.mp4'\n",
    "submission.rename(columns={'file':'filename','model_label':'label'}, inplace=True)\n",
    "submission.fillna(0.5)\n",
    "submission.label[submission.label < .07] = .07\n",
    "submission.label[submission.label > .93] = .93\n",
    "submission = submission.reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
